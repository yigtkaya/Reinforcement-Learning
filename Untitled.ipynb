{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b057a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import serial\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8065ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroSwimmerV0(gym.Env):\n",
    "    # Define the initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(9)\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'agent_location': gym.spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32),\n",
    "            'target_location': gym.spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.pool_width = 10\n",
    "        self.pool_height = 10\n",
    "        self.target_location = np.array([random.randint(5, 10), random.randint(5, 10)])\n",
    "        self.agent_location = np.array([2, 2])\n",
    "        self.max_episode_steps = 20\n",
    "        self.steps = 0\n",
    "\n",
    "    # Define the step function that takes an action as input and returns the next state, reward, done, and info\n",
    "    def step(self, action):\n",
    "\n",
    "        # calculate distance to target as previous distance\n",
    "        previous_distance = np.linalg.norm(self.agent_location - self.target_location)\n",
    "\n",
    "        # move microswimmer with action\n",
    "        movement = self.get_movement_from_action(action)\n",
    "\n",
    "        self.agent_location = self.agent_location + movement\n",
    "\n",
    "        # calculate distance to target as current distance\n",
    "        current_distance = np.linalg.norm(self.agent_location - self.target_location)\n",
    "\n",
    "        # calculate reward\n",
    "        reward = self.calculate_reward(previous_distance, current_distance)\n",
    "\n",
    "        # update steps\n",
    "        self.steps += 1\n",
    "\n",
    "        # determine if episode is terminated\n",
    "        done = (current_distance < 1.5) or (self.steps >= self.max_episode_steps) or (current_distance == 0)\n",
    "\n",
    "        # return next state, reward, done, and info\n",
    "\n",
    "        new_obs = {\n",
    "            'agent_location': self.agent_location,\n",
    "            'target_location': self.target_location\n",
    "        }\n",
    "\n",
    "        return new_obs, reward, done, {}\n",
    "\n",
    "    # Define the calculate_reward function\n",
    "    def calculate_reward(self, previous_distance, current_distance):\n",
    "\n",
    "        if current_distance < previous_distance:\n",
    "            reward = 2\n",
    "        elif current_distance == previous_distance:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = -2\n",
    "\n",
    "        if current_distance < 1.5:\n",
    "            reward = 15\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Define the get_movement_from_action function\n",
    "    def get_movement_from_action(self, action):\n",
    "        action_scalar = int(action)  # Convert action to integer\n",
    "        movement_dict = {\n",
    "            0: np.array([0, 0]),  # no movement x\n",
    "            1: np.array([0, 1]),  # down d\n",
    "            2: np.array([1, 1]),  # up-right e\n",
    "            3: np.array([1, 0]),  # right r\n",
    "            4: np.array([1, -1]),  # down-right c\n",
    "            5: np.array([0, -1]),  # up u\n",
    "            6: np.array([-1, -1]),  # down-left z\n",
    "            7: np.array([-1, 0]),  # left l\n",
    "            8: np.array([-1, 1])  # up-left q\n",
    "        }\n",
    "        return movement_dict[action_scalar]\n",
    "\n",
    "    # Define the render function\n",
    "    def render(self):\n",
    "\n",
    "        pool_size = 10\n",
    "        # Create an empty pool image\n",
    "        # print episode, step, and score information\n",
    "        pool_img = np.zeros((self.pool_height, self.pool_width))\n",
    "\n",
    "        # Draw the target location\n",
    "        target_pos = (int(self.target_location[0]), int(self.target_location[1]))\n",
    "        pool_img[target_pos[1], target_pos[0]] = 1.0\n",
    "\n",
    "        # Convert the continuous position to integer indices\n",
    "        microswimmer_pos = (int(self.agent_location[0]), int(self.agent_location[1]))\n",
    "        microswimmer_pos = np.clip(microswimmer_pos, 0, pool_size - 1)  # Clip the indices to valid range\n",
    "\n",
    "        # Draw the current location of the microswimmer\n",
    "        pool_img[microswimmer_pos[1], microswimmer_pos[0]] = 0.5\n",
    "\n",
    "        # Print the pool image\n",
    "        for row in pool_img:\n",
    "            for val in row:\n",
    "                if val == 1.0:\n",
    "                    print(\"T\", end=\" \")  # Target location\n",
    "                elif val == 0.5:\n",
    "                    print(\"M\", end=\" \")  # Microswimmer location\n",
    "                else:\n",
    "                    print(\".\", end=\" \")  # Empty space\n",
    "            print()\n",
    "\n",
    "    # Define the reset function\n",
    "    def reset(self):\n",
    "\n",
    "        self.agent_location = np.array([random.randint(0,9), random.randint(0,9)])\n",
    "        self.target_location = np.array([random.randint(0, 9), random.randint(0, 9)])\n",
    "\n",
    "        return {\n",
    "            'agent_location': self.agent_location,\n",
    "            'target_location': self.target_location\n",
    "        }\n",
    "\n",
    "    # Define the close function\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b6d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_action_to_arduino(act):\n",
    "    # send action to arduino\n",
    "    # ser = serial.Serial('/dev/cu.usbserial-10', 9600, timeout=1)\n",
    "    direction = get_direction_from_action(act)\n",
    "    # ser.write(direction.encode())\n",
    "\n",
    "    print(direction)\n",
    "\n",
    "\n",
    "def get_direction_from_action(action):\n",
    "    action_scalar = int(action)  # Convert action to integer\n",
    "    movement_dict = {\n",
    "        0: \"x\",\n",
    "        1: \"d\",\n",
    "        2: \"e\",\n",
    "        3: \"r\",\n",
    "        4: \"c\",\n",
    "        5: \"u\",\n",
    "        6: \"z\",\n",
    "        7: \"l\",\n",
    "        8: \"q\"\n",
    "    }\n",
    "    return movement_dict[int(action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cd31af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m MicroSwimmerV0()\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2D_Env\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m mean_reward, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:263\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    256\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:321\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[1;32m    314\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 321\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:304\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    301\u001b[0m     pos \u001b[38;5;241m=\u001b[39m (replay_buffer\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39mbuffer_size\n\u001b[1;32m    302\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mdones[pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:412\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:86\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     85\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:115\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "env = MicroSwimmerV0()\n",
    "\n",
    "model = DQN('MultiInputPolicy', env, verbose=1, learning_rate=1e-3)\n",
    "model.learn(total_timesteps=100000)\n",
    "model.save(\"2D_Env\")\n",
    "\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d687a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
